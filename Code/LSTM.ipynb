{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1a1835eb670>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import copy\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "plt.ioff() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n高精度、可靠的风速预报是气象学家面临的挑战。由对流风暴引起的强风，\\n造成相当大的破坏(大规模森林破坏.停电、建筑物/房屋损坏等)。\\n雷暴、龙卷风以及大冰雹、强风等对流事件是有可能扰乱日常生活的自然灾害，\\n特别是在有利于对流启动的复杂地形上。即使是普通的对流事件也会产生强风，\\n造成致命和昂贵的损失。因此，风速预测是一项重要的工作。\\n===本代码考虑，wind多因素的预测===\\n7个变量预测7个变量\\n步长：96\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "高精度、可靠的风速预报是气象学家面临的挑战。由对流风暴引起的强风，\n",
    "造成相当大的破坏(大规模森林破坏.停电、建筑物/房屋损坏等)。\n",
    "雷暴、龙卷风以及大冰雹、强风等对流事件是有可能扰乱日常生活的自然灾害，\n",
    "特别是在有利于对流启动的复杂地形上。即使是普通的对流事件也会产生强风，\n",
    "造成致命和昂贵的损失。因此，风速预测是一项重要的工作。\n",
    "===本代码考虑，wind多因素的预测===\n",
    "7个变量预测7个变量\n",
    "步长：96\n",
    "'''\n",
    "\n",
    "\n",
    "class Config():\n",
    "    data_path = 'D:/Study/机器学习/Project/ETTh1.csv'   # 数据path\n",
    "    timestep = 96  # 时间步长，就是利用多少时间窗口\n",
    "    batch_size = 32  # 批次大小\n",
    "    feature_size = 7  # 每个步长对应的特征数量，这里使用所有变量\n",
    "    hidden_size = 256  # 隐层大小\n",
    "    output_size = 7  # 预测变量的数量\n",
    "    pred_window = 96\n",
    "    num_layers = 2  # gru的层数\n",
    "    epochs = 2  # 迭代轮数\n",
    "    best_loss = 1008611  # 记录损失\n",
    "    learning_rate = 1e-3  # 学习率\n",
    "    model_name = 'lstm'  # 模型名称\n",
    "    save_path = '{}.pth'.format(model_name)  # 最优模型保存路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形成训练数据，例如12345789 12-3456789\n",
    "def split_data(data, timestep, feature_size, pred_window):\n",
    "    dataX = []  # 保存X\n",
    "    dataY = []  # 保存Y\n",
    "\n",
    "    # 将整个窗口的数据保存到X中，将未来96/336保存到Y中\n",
    "    for index in range(len(data) - timestep - pred_window):\n",
    "        dataX.append(data[index: index + timestep][:, :])  #\n",
    "        dataY.append(data[index + timestep: index + timestep + pred_window][:, ])  #\n",
    "\n",
    "    dataX = np.array(dataX)\n",
    "    dataY = np.array(dataY)\n",
    "\n",
    "    # 获取训练集大小\n",
    "    train_size = int(np.round(0.6 * dataX.shape[0]))\n",
    "    validation_size = int(np.round(0.8 * dataX.shape[0]))\n",
    "\n",
    "    # 划分训练集、测试集、验证集\n",
    "    x_train = dataX[: train_size, :].reshape(-1, timestep, feature_size)\n",
    "    y_train = dataY[: train_size, :].reshape(-1, pred_window, feature_size)\n",
    "\n",
    "    x_test = dataX[train_size:validation_size, :].reshape(-1, timestep, feature_size)\n",
    "    y_test = dataY[train_size:validation_size, :].reshape(-1, pred_window, feature_size)\n",
    "\n",
    "    x_val = dataX[validation_size:, :].reshape(-1, timestep, feature_size)\n",
    "    y_val = dataY[validation_size:, :].reshape(-1, pred_window, feature_size)\n",
    "\n",
    "    return [x_train, y_train, x_test, y_test, x_val, y_val]\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, pred_window):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # LSTM layer\n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        output, _ = self.lstm(x, (h_0, c_0))\n",
    "        predictions = []\n",
    "        for i in range(pred_window):\n",
    "            if i < seq_len:\n",
    "                out = self.fc(output[:, i, :])\n",
    "            else:\n",
    "                out = self.fc(output[:, -1, :])  # Use the last available time step for prediction\n",
    "            predictions.append(out.unsqueeze(1))\n",
    "\n",
    "        predictions = torch.cat(predictions, dim=1)\n",
    "        return predictions\n",
    "\n",
    "def plot_prediction(scaler2, y_test_pred, y_test, pred_window, id):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(scaler2.inverse_transform(y_test_pred[0][:, -1].detach().numpy().reshape(-1, 1)), color=\"b\", label='predict')\n",
    "    plt.plot(scaler2.inverse_transform(y_test[0][:, -1].detach().numpy().reshape(-1, 1)), \"r\", label='real')\n",
    "    plt.xlabel('hours', fontsize=12)\n",
    "    plt.ylabel('oil temperature', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.savefig('./Pic/oil_prediction' + pred_window + id + '-Pic.png', format='png')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69101763],\n",
       "       [0.63623297],\n",
       "       [0.63623297],\n",
       "       ...,\n",
       "       [0.28652145],\n",
       "       [0.27667858],\n",
       "       [0.27246592]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "config.learning_rate = 0.01\n",
    "config.batch_size = 32\n",
    "config.hidden_size = 64\n",
    "config.num_layers = 4\n",
    "df = pd.read_csv(config.data_path, index_col=0)\n",
    "# df\n",
    "scaler = MinMaxScaler()\n",
    "# np.array(df)\n",
    "data = scaler.fit_transform(np.array(df))\n",
    "# data\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.fit_transform(np.array(df['OT']).reshape(-1, 1))\n",
    "x_train, y_train, x_test, y_test, x_val, y_val = split_data(data, config.timestep, config.feature_size, config.pred_window)\n",
    "\n",
    "# 转为tensor\n",
    "x_train_tensor = torch.from_numpy(x_train).to(torch.float32).to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).to(torch.float32).to(device)\n",
    "x_test_tensor = torch.from_numpy(x_test).to(torch.float32).to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test).to(torch.float32).to(device)\n",
    "x_val_tensor = torch.from_numpy(x_val).to(torch.float32).to(device)\n",
    "y_val_tensor = torch.from_numpy(y_val).to(torch.float32).to(device)\n",
    "\n",
    "# 形成数据集\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "val_data = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "# 将数据集加载为迭代器\n",
    "train_loader = torch.utils.data.DataLoader(train_data, config.batch_size, True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, config.batch_size, True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, config.batch_size, True)\n",
    "\n",
    "model = LSTMModel(config.feature_size, config.hidden_size, \n",
    "                  config.output_size, config.num_layers)\n",
    "                  \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1f4bec1f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 10/108 [00:01<00:16,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1f4bec1f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 10/108 [00:01<00:15,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1f4bec1f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/108 [00:00<?, ?it/s]C:\\Users\\fu\\AppData\\Local\\Temp\\ipykernel_71160\\3958272285.py:58: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(12, 8))\n",
      "  9%|▉         | 10/108 [00:01<00:15,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1f4bec1f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 10/108 [00:01<00:14,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(config, train=True):\n",
    "    loss_mse = nn.MSELoss()  # 定义损失函数\n",
    "    loss_mae = nn.L1Loss()\n",
    "    l = []\n",
    "    l2 = []\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)  # 定义优化器\n",
    "    # 8.模型训练\n",
    "    if train:\n",
    "        for epoch in range(config.epochs):\n",
    "            running_loss = 0\n",
    "            # train_bar = tqdm(train_loader)  # 形成进度条\n",
    "            train_bar = tqdm(train_loader)  # 验证集，确定超参数\n",
    "            for data in train_bar:\n",
    "                x_train, y_train = data  # 解包迭代器中的X和Y;(32,96,7),(32,96)\n",
    "                optimizer.zero_grad()\n",
    "                y_train_pred = model(x_train, config.pred_window)\n",
    "                loss = loss_mse(y_train_pred, y_train)\n",
    "                loss2 = loss_mae(y_train_pred, y_train)\n",
    "                l.append(loss.item())\n",
    "                l2.append(loss2.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
    "                                                                         config.epochs,\n",
    "                                                                         loss)\n",
    "        torch.save(model.state_dict(), config.save_path)\n",
    "        # 打印loss mse和loss mae\n",
    "        # 创建图表\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        # 绘制数据\n",
    "        plt.plot(l, label='MSE')\n",
    "        plt.plot(l2, label='MAE')\n",
    "        # 添加标签、标题等\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title(\"MSE's and MAE's loss curve\")\n",
    "        plt.legend()  # 显示图例\n",
    "        # 显示图表\n",
    "        plt.grid(True)  # 添加网格线\n",
    "        plt.savefig('LSTM-MAEandMSE-Pic.png', format='png')\n",
    "        plt.show()\n",
    "\n",
    "        return loss.detach().numpy(), loss2.detach().numpy()\n",
    "    \n",
    "    else:  # 预测，加载模型\n",
    "        # 模型验证\n",
    "        # 加载已保存的模型参数\n",
    "        model.load_state_dict(torch.load(config.save_path))\n",
    "        model.eval()  # 如果只是用来预测，而非继续训练，需要调用eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            test_bar = tqdm(test_loader)\n",
    "            id = 1\n",
    "            for data in test_bar:\n",
    "                if id > 10:\n",
    "                    break\n",
    "                x_test, y_test = data\n",
    "                y_test_pred = model(x_test, config.pred_window)\n",
    "                # debug_var = y_test_pred[0]\n",
    "                # debug_var2 = y_test[0]\n",
    "                plot_prediction(scaler2, y_test_pred, y_test, str(config.pred_window), str(id))\n",
    "                id += 1\n",
    "                  \n",
    "            test_loss = loss_mse(y_test_pred, y_test)\n",
    "\n",
    "        if test_loss < config.best_loss:\n",
    "            config.best_loss = test_loss\n",
    "            torch.save(model.state_dict(), config.save_path)\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    config.learning_rate = 0.01\n",
    "    config.batch_size = 16\n",
    "    config.hidden_size = 64\n",
    "    config.num_layers = 4\n",
    "    config.pred_window = 96\n",
    "    # train(config, True)\n",
    "    for seed in [2, 23, 98, 1024]:  # , 5467, 20231225\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        train(config, False)\n",
    "        # pd.DataFrame({'seed': [seed], 'mse': [mse], 'mae': [mae], 'std': [std]}).to_csv('train_loss.csv', mode='a', header=False, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyfpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
